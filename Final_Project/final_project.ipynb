{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 import packages\n",
    "\n",
    "import sys\n",
    "import string\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi mappings from groove dataset\n",
    "# kick drum\n",
    "BASS = 36\n",
    "# snare drum\n",
    "SNARE_HEAD = 38\n",
    "SNARE_RIM = 40\n",
    "SNARE_X_STICK = 37\n",
    "# toms\n",
    "TOM_1_HEAD = 48\n",
    "TOM_1_RIM = 50\n",
    "TOM_2_HEAD = 45\n",
    "TOM_2_RIM = 47\n",
    "TOM_3_HEAD = 43\n",
    "TOM_3_RIM = 58\n",
    "# hi-hats\n",
    "HH_OPEN_BOW = 46\n",
    "HH_OPEN_EDGE = 26\n",
    "HH_CLOSED_BOW = 42\n",
    "HH_CLOSED_EDGE = 22\n",
    "HH_PEDAL = 44\n",
    "# crash cymbal\n",
    "CRASH_1_BOW = 49\n",
    "CRASH_1_EDGE = 55\n",
    "CRASH_2_BOW = 57\n",
    "CRASH_2_EDGE = 52\n",
    "# ride cymbal\n",
    "RIDE_BOW = 51\n",
    "RIDE_EDGE = 59\n",
    "RIDE_BELL = 53\n",
    "\n",
    "# mappings for our own training (9)\n",
    "KICK = 0\n",
    "SNARE = 1\n",
    "HH_OPEN = 2\n",
    "HH_CLOSED = 3\n",
    "RIDE = 4\n",
    "TOM_1 = 5\n",
    "TOM_2 = 6\n",
    "TOM_3 = 7\n",
    "CRASH = 8\n",
    "\n",
    "# groove mappings to our mappings\n",
    "KICK_LIST = [BASS]\n",
    "SNARE_LIST = [SNARE_HEAD, SNARE_RIM, SNARE_X_STICK]\n",
    "HH_OPEN_LIST = [HH_OPEN_BOW, HH_OPEN_EDGE]\n",
    "HH_CLOSED_LIST = [HH_CLOSED_BOW, HH_CLOSED_EDGE, HH_PEDAL]\n",
    "RIDE_LIST = [RIDE_BOW, RIDE_EDGE, RIDE_BELL]\n",
    "TOM_1_LIST = [TOM_1_HEAD, TOM_1_RIM]\n",
    "TOM_2_LIST = [TOM_2_HEAD, TOM_2_RIM]\n",
    "TOM_3_LIST = [TOM_3_HEAD, TOM_3_RIM]\n",
    "CRASH_LIST = [CRASH_1_BOW, CRASH_1_EDGE, CRASH_2_BOW, CRASH_2_EDGE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #1.1 GPU stuff\n",
    "\n",
    "# print (\"cuda: \", torch.cuda.is_available())\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print (\"current device: \", device)\n",
    "# print (\"count: \", torch.cuda.device_count())\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print (\"device name: \", torch.cuda.get_device_name(0))\n",
    "#     torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 load groove dataset\n",
    "import math\n",
    "\n",
    "groove_csv = pd.read_csv('groove/info.csv')\n",
    "print(\"groove dataset:\", len(groove_csv))\n",
    "\n",
    "# get train, test, and validation sets\n",
    "train_csv = []\n",
    "test_csv = []\n",
    "validation_csv = []\n",
    "\n",
    "for index, row in groove_csv.iterrows():\n",
    "    if str(row.audio_filename).lower() != \"nan\":\n",
    "        split = row['split']\n",
    "        if split == \"train\":\n",
    "            train_csv.append(row)\n",
    "        elif split == \"test\":\n",
    "            test_csv.append(row)\n",
    "        elif split == \"validation\":\n",
    "            validation_csv.append(row)\n",
    "        \n",
    "print (\"train: \", len(train_csv))\n",
    "print (\"test: \", len(test_csv))\n",
    "print (\"validation: \", len(validation_csv))\n",
    "\n",
    "print (train_csv[0].midi_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "# code to convert midi file to array\n",
    "# https://medium.com/analytics-vidhya/convert-midi-file-to-numpy-array-in-python-7d00531890c\n",
    "def msg2dict(msg):\n",
    "    result = dict()\n",
    "    if 'note_on' in msg:\n",
    "        on_ = True\n",
    "    elif 'note_off' in msg:\n",
    "        on_ = False\n",
    "    else:\n",
    "        on_ = None\n",
    "    result['time'] = int(msg[msg.rfind('time'):].split(' ')[0].split('=')[1].translate(\n",
    "        str.maketrans({a: None for a in string.punctuation})))\n",
    "\n",
    "    if on_ is not None:\n",
    "        for k in ['note', 'velocity']:\n",
    "            result[k] = int(msg[msg.rfind(k):].split(' ')[0].split('=')[1].translate(\n",
    "                str.maketrans({a: None for a in string.punctuation})))\n",
    "    return [result, on_]\n",
    "\n",
    "def switch_note(last_state, note, velocity, on_=True):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of this range will be ignored\n",
    "    result = [0] * 88 if last_state is None else last_state.copy()\n",
    "    if 21 <= note <= 108:\n",
    "        value = 1 if velocity > 0 else 0\n",
    "        result[note-21] = value if on_ else 0\n",
    "    return result\n",
    "\n",
    "def get_new_state(new_msg, last_state):\n",
    "    new_msg, on_ = msg2dict(str(new_msg))\n",
    "    new_state = switch_note(last_state, note=new_msg['note'], velocity=new_msg['velocity'], on_=on_) if on_ is not None else last_state\n",
    "    return [new_state, new_msg['time']]\n",
    "\n",
    "def track2seq(track):\n",
    "    # piano has 88 notes, corresponding to note id 21 to 108, any note out of the id range will be ignored\n",
    "    result = []\n",
    "    last_state, last_time = get_new_state(str(track[0]), [0]*88)\n",
    "    for i in range(1, len(track)):\n",
    "        new_state, new_time = get_new_state(track[i], last_state)\n",
    "        if new_time > 0:\n",
    "            result += [last_state]*new_time\n",
    "        last_state, last_time = new_state, new_time\n",
    "    return result\n",
    "\n",
    "def mid2array(mid, min_msg_pct=0.1):\n",
    "    tracks_len = [len(tr) for tr in mid.tracks]\n",
    "    min_n_msg = max(tracks_len) * min_msg_pct\n",
    "    # convert each track to nested list\n",
    "    all_arys = []\n",
    "    for i in range(len(mid.tracks)):\n",
    "        if len(mid.tracks[i]) > min_n_msg:\n",
    "            ary_i = track2seq(mid.tracks[i])\n",
    "            all_arys.append(ary_i)\n",
    "    # make all nested list the same length\n",
    "    max_len = max([len(ary) for ary in all_arys])\n",
    "    for i in range(len(all_arys)):\n",
    "        if len(all_arys[i]) < max_len:\n",
    "            all_arys[i] += [[0] * 88] * (max_len - len(all_arys[i]))\n",
    "    all_arys = np.array(all_arys)\n",
    "    all_arys = all_arys.max(axis=0)\n",
    "    # trim: remove consecutive 0s in the beginning and at the end\n",
    "    # sums = all_arys.sum(axis=1)\n",
    "    # ends = np.where(sums > 0)[0]\n",
    "    return all_arys #[min(ends): max(ends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert audio files into tensors\n",
    "import imp\n",
    "from scipy import signal\n",
    "import audiosegment\n",
    "import librosa\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# converts an audio file to tensor\n",
    "def audio_to_melspec_tensor(wav_file_path, sample_rate=44_100): \n",
    "    window_size = 0.025\n",
    "    window_stride = 0.01\n",
    "    n_dft = int(sample_rate * window_size)\n",
    "    n_mels = 128\n",
    "    win_length = 1024\n",
    "    hop_length = int(sample_rate * window_stride)\n",
    "    # load in wav file and remove the mean of the signal\n",
    "    y, sr = librosa.load(wav_file_path, sr=sample_rate)\n",
    "    y = y - y.mean()\n",
    "    y = np.append(y[0],y[1:]-.97*y[:-1])\n",
    "    # compute mel spectrogram\n",
    "    stft = librosa.stft(y, n_fft=n_dft, hop_length=hop_length, win_length=win_length, window=signal.hamming)\n",
    "    spec = np.abs(stft)**2\n",
    "    mel_basis = librosa.filters.mel(sr=sample_rate, n_fft=n_dft, n_mels=n_mels, fmin=20)\n",
    "    melspec = np.dot(mel_basis, spec)\n",
    "    logspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    logspec = np.transpose(logspec)\n",
    "    # plot.imshow(logspec.T, origin='lower', aspect='auto')\n",
    "    # plot.show()\n",
    "    # turn into tensor\n",
    "    logspec_tensor = torch.tensor(logspec)\n",
    "    return logspec_tensor\n",
    "\n",
    "def reformat_midi_for_training(midi_array):\n",
    "    reformated_midi_array = np.zeros((midi_array.shape[0], 9)) # create new array with only 9 possible notes\n",
    "    for note in range(midi_array.shape[1]): # for every possible note value (0-88) (need to add +21 to get correct note value)\n",
    "        correct_note = note + 21\n",
    "        for t in range(midi_array.shape[0]): # for every time step\n",
    "            if (midi_array[t][note] != 0):\n",
    "                # check each list\n",
    "                if correct_note in KICK_LIST:\n",
    "                    reformated_midi_array[t][KICK] = 1\n",
    "                elif correct_note in SNARE_LIST:\n",
    "                    reformated_midi_array[t][SNARE] = 1\n",
    "                elif correct_note in HH_OPEN_LIST:\n",
    "                    reformated_midi_array[t][HH_OPEN] = 1\n",
    "                elif correct_note in HH_CLOSED_LIST:\n",
    "                    reformated_midi_array[t][HH_CLOSED] = 1\n",
    "                elif correct_note in RIDE_LIST:\n",
    "                    reformated_midi_array[t][RIDE] = 1\n",
    "                elif correct_note in TOM_1_LIST:\n",
    "                    reformated_midi_array[t][TOM_1] = 1\n",
    "                elif correct_note in TOM_2_LIST:\n",
    "                    reformated_midi_array[t][TOM_2] = 1\n",
    "                elif correct_note in TOM_3_LIST:\n",
    "                    reformated_midi_array[t][TOM_3] = 1\n",
    "                elif correct_note in CRASH_LIST:\n",
    "                    reformated_midi_array[t][CRASH] = 1\n",
    "    return reformated_midi_array\n",
    "\n",
    "# shrinks an array down keeping the distance between values propotional\n",
    "def shrink_array_proportionally(midi_array, target_resize):\n",
    "    resized_array = np.zeros((target_resize, midi_array.shape[1]))\n",
    "    ratio = midi_array.shape[0] / target_resize\n",
    "    # iterate through each timestep\n",
    "    for t in range(midi_array.shape[0]): \n",
    "        # for each value\n",
    "        for i in range(midi_array.shape[1]):\n",
    "            value = midi_array[t][i]\n",
    "            if value > 0:\n",
    "                t2 = int(t / ratio)\n",
    "                resized_array[t2][i] = value\n",
    "    return resized_array\n",
    "\n",
    "# converts a midi array to a list of tensors\n",
    "def midi_to_tensors(midi_array, num_tensors):\n",
    "    midi_tensors = []\n",
    "    split_midi_arrays = np.array_split(midi_array, num_tensors)\n",
    "    for i in range(len(split_midi_arrays)):\n",
    "        midi_tensors.append(torch.tensor(split_midi_arrays[i]))\n",
    "    return midi_tensors\n",
    "\n",
    "def get_feats_and_labels_from_csv(csv_index):\n",
    "    # load in wav file\n",
    "    audio_file_path = \"groove/\" + csv_index.audio_filename\n",
    "    wav_file = audiosegment.from_file(audio_file_path)\n",
    "    # convert sample width if not set to 2 (16 bits)\n",
    "    if wav_file.sample_width != 2:\n",
    "        wav_file = wav_file.set_sample_width(2)\n",
    "        # print(\"\\tnew sample_width: \", wav_file.sample_width)\n",
    "        wav_file.export(audio_file_path, format=\"wav\")\n",
    "    # convert file from stereo to mono if channels > 1\n",
    "    if wav_file.channels != 1:\n",
    "        wav_file = wav_file.set_channels(1)\n",
    "        wav_file.export(audio_file_path, format=\"wav\")\n",
    "    # cutting and padding\n",
    "    predefined_length = 9.99 # had to make a bit smaller than 10 sec. because was sizing tensor to 1001 instead of 1000\n",
    "    tensor_size = 1000 # used for label tensor creation\n",
    "    diced_wav_files = wav_file.dice(predefined_length, zero_pad=True)\n",
    "    # get feature tensors\n",
    "    default_sample_rate = 44100\n",
    "    target_len = predefined_length * default_sample_rate\n",
    "    feats_tensors_list = []\n",
    "    i = 0\n",
    "    for diced_file in diced_wav_files:\n",
    "        # pad with zeros if not correct length\n",
    "        diced_file_len = len(diced_file.to_numpy_array())\n",
    "        if diced_file_len != target_len:\n",
    "            zeros = int(target_len - diced_file_len)\n",
    "            diced_array = np.pad(diced_file.to_numpy_array(), (0, zeros))\n",
    "            diced_file = audiosegment.from_numpy_array(diced_array, framerate=default_sample_rate)\n",
    "        # export temp wav file and convert to tensor\n",
    "        diced_file_path = str(csv_index.id) + \"-\" + str(i) + \".wav\"\n",
    "        diced_file_path = diced_file_path.replace('/', '-')\n",
    "        diced_file_path = \"temp/\" + diced_file_path\n",
    "        diced_file.export(diced_file_path, format=\"wav\")\n",
    "        feats_tensor = audio_to_melspec_tensor(diced_file_path, wav_file.frame_rate)\n",
    "        feats_tensors_list.append(feats_tensor)\n",
    "        i += 1\n",
    "        \n",
    "    # load in midi file\n",
    "    midi_file_path = \"groove/\" + csv_index.midi_filename\n",
    "    midi = mido.MidiFile(midi_file_path)\n",
    "\n",
    "    # convert midi to arrray\n",
    "    midi_array = mid2array(midi)\n",
    "    # f = plot.figure()\n",
    "    # f.set_figwidth(20)\n",
    "    # f.set_figheight(10)\n",
    "    # plot.plot(range(midi_array.shape[0]), np.multiply(np.where(midi_array > 0, 1, 0), range(1, 89)), marker='.', markersize=1, linestyle='')\n",
    "    # plot.title(\"midi\")\n",
    "    # plot.show()\n",
    "\n",
    "    # reformat midi array for training\n",
    "    midi_array = reformat_midi_for_training(midi_array)\n",
    "    # f = plot.figure()\n",
    "    # f.set_figwidth(20)\n",
    "    # f.set_figheight(10)\n",
    "    # plot.plot(range(midi_array.shape[0]), np.multiply(np.where(midi_array > 0, 1, 0), range(1, 10)), marker='.', markersize=1, linestyle='')\n",
    "    # plot.title(\"reformated midi\")\n",
    "    # plot.ylabel('kick          snare          hh_open          hh_closed          ride          tom1          tom2          tom3          crash')\n",
    "    # plot.show()\n",
    "\n",
    "    # resize array to be directly related to duration of the audio file (every second is 100 units)\n",
    "    target_resize = int(csv_index.duration * 100)\n",
    "    #print (\"current midi array size: \", midi_array.shape[0], \" target size: \", target_resize)\n",
    "    midi_array = shrink_array_proportionally(midi_array, target_resize)\n",
    "    #print (\"shrinked midi_array.shape: \", midi_array.shape)\n",
    "    # f = plot.figure()\n",
    "    # f.set_figwidth(20)\n",
    "    # f.set_figheight(10)\n",
    "    # plot.plot(range(midi_array.shape[0]), np.multiply(np.where(midi_array > 0, 1, 0), range(1, 10)), marker='.', markersize=1, linestyle='')\n",
    "    # plot.title(\"shrinked midi\")\n",
    "    # plot.ylabel('kick          snare          hh_open          hh_closed          ride          tom1          tom2          tom3          crash')\n",
    "    # plot.show()\n",
    "\n",
    "    # pad midi array to be the same size as \n",
    "    zeros = int((tensor_size * len(feats_tensors_list)) - len(midi_array))\n",
    "    zeros_pad = np.zeros((zeros, 9))\n",
    "    midi_array = np.concatenate((midi_array, zeros_pad))\n",
    "    #print (\"midi_array.shape: \", midi_array.shape)\n",
    "\n",
    "    # create label tensors\n",
    "    label_tensors_list = midi_to_tensors(midi_array, len(feats_tensors_list))\n",
    "\n",
    "    # return tensor lists\n",
    "    return feats_tensors_list , label_tensors_list\n",
    "\n",
    "# reset temp folder\n",
    "if os.path.isfile('temp/'):\n",
    "    shutil.rmtree('temp/', ignore_errors=True)\n",
    "if not os.path.exists('temp/'):\n",
    "    os.mkdir('temp/')\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for index in train_csv:\n",
    "    print (i, \" \", index.audio_filename)\n",
    "    feats_tensors, label_tensors = get_feats_and_labels_from_csv(index)\n",
    "    for i in range(len(feats_tensors)):\n",
    "        print (\"\\tfeat tensor: \", feats_tensors[i].shape, \" label tensor: \", label_tensors[i].shape)\n",
    "\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2eb717040054f9e1cd6390da57b4c3f6de62338193843f816e8f12a4d5407ba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
