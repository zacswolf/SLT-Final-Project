{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/ihsuan00/SLT-Final-Project/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 0.0 import packages\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numba import cuda as numba\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device:  GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "# 0.2 GPU stuff\n",
    "device_num = 1\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"torch device: \", torch.cuda.get_device_name())\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# function to clear GPU memory\n",
    "def free_gpu_cache():                        \n",
    "    torch.cuda.empty_cache()\n",
    "    numba.select_device(device_num)\n",
    "    numba.close()\n",
    "    numba.select_device(device_num)\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 load in datasets\n",
    "\n",
    "def load_datasets(print_out = False, train_device=\"cpu\", val_device=\"cpu\", test_device=\"cpu\"):\n",
    "    # load files\n",
    "    train_feats = np.load(\"data/train_feats.npy\", allow_pickle=True)\n",
    "    train_labels = np.load(\"data/train_labels.npy\", allow_pickle=True)\n",
    "    test_feats = np.load(\"data/test_feats.npy\", allow_pickle=True)\n",
    "    test_labels = np.load(\"data/test_labels.npy\", allow_pickle=True)\n",
    "    val_feats = np.load(\"data/val_feats.npy\", allow_pickle=True)\n",
    "    val_labels = np.load(\"data/val_labels.npy\", allow_pickle=True)\n",
    "    # reshape numpy arrays\n",
    "    train_feats = train_feats.reshape(train_feats.shape[0]*train_feats.shape[1], train_feats.shape[2])\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0]*train_labels.shape[1], train_labels.shape[2])\n",
    "    test_feats = test_feats.reshape(test_feats.shape[0]*test_feats.shape[1], test_feats.shape[2])\n",
    "    test_labels = test_labels.reshape(test_labels.shape[0]*test_labels.shape[1], test_labels.shape[2])\n",
    "    val_feats = val_feats.reshape(val_feats.shape[0]*val_feats.shape[1], val_feats.shape[2])\n",
    "    val_labels = val_labels.reshape(val_labels.shape[0]*val_labels.shape[1], val_labels.shape[2])\n",
    "    \n",
    "    if print_out:\n",
    "        print(\"train feats: \", train_feats.shape, \" type: \", type(train_feats))\n",
    "        print(\"train labels: \", train_labels.shape, \" type: \", type(train_labels))\n",
    "\n",
    "        print(\"test feats: \", test_feats.shape, \" type: \", type(test_feats))\n",
    "        print(\"test labels: \", test_labels.shape, \" type: \", type(test_labels))\n",
    "\n",
    "        print(\"val feats: \", val_feats.shape, \" type: \", type(val_feats))\n",
    "        print(\"val labels: \", val_labels.shape, \" type: \", type(val_labels))\n",
    "\n",
    "    # create tensors\n",
    "    train_feats_tensor = torch.tensor(train_feats, requires_grad=True, dtype=torch.float).to(train_device)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.float).to(train_device)\n",
    "\n",
    "    test_feats_tensor = torch.tensor(test_feats, requires_grad=True, dtype=torch.float).to(test_device)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.float).to(test_device)\n",
    "\n",
    "    val_feats_tensor = torch.tensor(val_feats, requires_grad=True, dtype=torch.float).to(val_device)\n",
    "    val_labels_tensor = torch.tensor(val_labels, dtype=torch.float).to(val_device)\n",
    "\n",
    "    if print_out:\n",
    "        print (\"train feats tensor: \", train_feats_tensor.shape, \" type: \", type(train_feats_tensor))\n",
    "        print (\"train labels tensor: \", train_labels_tensor.shape, \" type: \", type(train_labels_tensor))\n",
    "\n",
    "        print (\"test feats tensor: \", test_feats_tensor.shape, \" type: \", type(test_feats_tensor))\n",
    "        print (\"test labels tensor: \", test_labels_tensor.shape, \" type: \", type(test_labels_tensor))\n",
    "\n",
    "        print (\"val feats tensor: \", val_feats_tensor.shape, \" type: \", type(val_feats_tensor))\n",
    "        print (\"val labels tensor: \", val_labels_tensor.shape, \" type: \", type(val_labels_tensor))\n",
    "            \n",
    "        if print_out:\n",
    "            print (\"train_feats_tensor.device:\", train_feats_tensor.get_device())\n",
    "            print (\"train_labels_tensor.device:\", train_labels_tensor.get_device())\n",
    "            print (\"test_feats_tensor.device:\", test_feats_tensor.get_device())\n",
    "            print (\"test_labels_tensor.device:\", test_labels_tensor.get_device())\n",
    "            print (\"val_feats_tensor.device:\", val_feats_tensor.get_device())\n",
    "            print (\"val_labels_tensor.device:\", val_labels_tensor.get_device())\n",
    "        \n",
    "    return  (train_feats_tensor,\n",
    "            train_labels_tensor,\n",
    "            test_feats_tensor,\n",
    "            test_labels_tensor,\n",
    "            val_feats_tensor,\n",
    "            val_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 set up the data loaders\n",
    "\n",
    "# tensor tuple shape: output of load_datasets()\n",
    "#   [ train_feats_tensor, train_labels_tensor,\n",
    "#     test_feats_tensor, test_labels_tensor,\n",
    "#     val_feats_tensor, val_labels_tensor ]\n",
    "def set_up_dataloaders(batch_size, tensor_tuple):\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(tensor_tuple[0], tensor_tuple[1])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(tensor_tuple[2], tensor_tuple[3])\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle = False)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(tensor_tuple[4], tensor_tuple[5])\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "    \n",
    "    return  (train_dataset, train_loader,\n",
    "            test_dataset, test_loader,\n",
    "            val_dataset, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 bi-LSTM Model Architecture\n",
    "\n",
    "class bi_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers=2, model_type='LSTM'):\n",
    "        super(bi_LSTM, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device) # hidden state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device) # cell state\n",
    "        \n",
    "        # print (\"h0 device: \", h0.device)\n",
    "        # print (\"c0 device: \", c0.device)\n",
    "        # print (\"x device: \", x.device)\n",
    "        \n",
    "        x = x[:, None, :]\n",
    "        #print (\"init x shape: \", x.shape)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #print (\"lstm out shape: \", out.shape)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #print (\"linear out shape: \", out.shape)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 helper functions for training\n",
    "\n",
    "def test_network(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # get data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            #print (\"labels: \", labels)\n",
    "            #print (\"pred: \", outputs)\n",
    "            total = labels.shape[0] * labels.shape[1]\n",
    "            correct = 0\n",
    "            for i, frame in enumerate(labels):\n",
    "                #print (i, \" frame: \", frame)\n",
    "                #print (i, \" outputs[i]: \", outputs[i])\n",
    "                for val in torch.eq(frame, outputs[i]):\n",
    "                    if val:\n",
    "                        correct += 1\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "def print_stats(iteration_list, accuracy_list, loss_list):\n",
    "    # final accuracy plot        \n",
    "    plt.plot(iteration_list, accuracy_list)\n",
    "    plt.title(\"accuracy over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # final loss plot        \n",
    "    plt.plot(iteration_list, loss_list)\n",
    "    plt.title(\"loss over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feats:  (3585000, 128)  type:  <class 'numpy.ndarray'>\n",
      "train labels:  (3585000, 9)  type:  <class 'numpy.ndarray'>\n",
      "test feats:  (497000, 128)  type:  <class 'numpy.ndarray'>\n",
      "test labels:  (497000, 9)  type:  <class 'numpy.ndarray'>\n",
      "val feats:  (518000, 128)  type:  <class 'numpy.ndarray'>\n",
      "val labels:  (518000, 9)  type:  <class 'numpy.ndarray'>\n",
      "train feats tensor:  torch.Size([3585000, 128])  type:  <class 'torch.Tensor'>\n",
      "train labels tensor:  torch.Size([3585000, 9])  type:  <class 'torch.Tensor'>\n",
      "test feats tensor:  torch.Size([497000, 128])  type:  <class 'torch.Tensor'>\n",
      "test labels tensor:  torch.Size([497000, 9])  type:  <class 'torch.Tensor'>\n",
      "val feats tensor:  torch.Size([518000, 128])  type:  <class 'torch.Tensor'>\n",
      "val labels tensor:  torch.Size([518000, 9])  type:  <class 'torch.Tensor'>\n",
      "train_feats_tensor.device: -1\n",
      "train_labels_tensor.device: -1\n",
      "test_feats_tensor.device: -1\n",
      "test_labels_tensor.device: -1\n",
      "val_feats_tensor.device: -1\n",
      "val_labels_tensor.device: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/112032 [09:07<906:53:07, 29.15s/it]"
     ]
    }
   ],
   "source": [
    "# 3.0 Training\n",
    "\n",
    "# free_gpu_cache()\n",
    "\n",
    "# model parameters:\n",
    "batch_size = 32\n",
    "input_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 9\n",
    "num_layers = 2\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# get dataloaders\n",
    "train_dataset, train_loader, test_dataset, test_loader, val_dataset, val_loader  = set_up_dataloaders(\n",
    "    batch_size, load_datasets(print_out=True)\n",
    ")\n",
    "\n",
    "# create model\n",
    "model = bi_LSTM(input_dim, hidden_dim, batch_size, output_dim, num_layers, 'LSTM')\n",
    "model.to(device)\n",
    "\n",
    "# print (\"device name: \", torch.cuda.get_device_name(0))\n",
    "# print (\"model.type: \", myModel.model_type)\n",
    "# print (\"model.device: \", next(myModel.parameters()).device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lists for data collection\n",
    "iter = 0\n",
    "delta = 100\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "# perform epochs\n",
    "startTime = time.time()\n",
    "min_valid_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for batch_index, (feats, labels) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        # place data on GPU\n",
    "        feats = feats.to(device).squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "        # print (\"feats shape: \", feats.shape)\n",
    "        # print (\"labels shape: \", labels.shape)\n",
    "        # print (\"labels: \", labels)\n",
    "        \n",
    "        # forward\n",
    "        output = model(feats)\n",
    "        loss = criterion(output, labels)\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        #gradient descent\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    valid_loss = 0.0\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch_index, (feats, labels) in enumerate(tqdm(val_loader)):\n",
    "        # Transfer Data to GPU if available\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(feats)\n",
    "        loss = criterion(output,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "    print(f'Epoch {epoch} \\t\\t Training Loss: {total_loss/ len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}') \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f\\\n",
    "        }--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "    \n",
    "    # print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %') \n",
    "    # print('Test accuracy: %d %%' % (100 * correct / total)) \n",
    "        # test accuracy and log stats\n",
    "        # if iter % delta == 0 and iter != 0:\n",
    "        #     print(\"Testing Network\")\n",
    "        #     acc = test_network(model, test_loader)\n",
    "        #     iteration_list.append(iter)\n",
    "        #     accuracy_list.append(acc)\n",
    "        #     loss_list.append(loss)\n",
    "        #     print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %')\n",
    "    \n",
    "        # # increase iteration\n",
    "        # iter += 1\n",
    "\n",
    "print (\"time elapsed: \", round((time.time() - startTime), 2), \" sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
