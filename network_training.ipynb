{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 import packages\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numba import cuda as numba\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 GPU stuff\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"torch device: \", torch.cuda.get_device_name())\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# function to clear GPU memory\n",
    "def free_gpu_cache():                        \n",
    "    torch.cuda.empty_cache()\n",
    "    numba.select_device(0)\n",
    "    numba.close()\n",
    "    numba.select_device(0)\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 load in datasets\n",
    "\n",
    "def load_datasets(print_out = False, place_on_gpu=True):\n",
    "    # load files\n",
    "    train_feats = np.load(\"data/train_feats.npy\", allow_pickle=True)\n",
    "    train_labels = np.load(\"data/train_labels.npy\", allow_pickle=True)\n",
    "    test_feats = np.load(\"data/test_feats.npy\", allow_pickle=True)\n",
    "    test_labels = np.load(\"data/test_labels.npy\", allow_pickle=True)\n",
    "    val_feats = np.load(\"data/val_feats.npy\", allow_pickle=True)\n",
    "    val_labels = np.load(\"data/val_labels.npy\", allow_pickle=True)\n",
    "    # reshape numpy arrays\n",
    "    train_feats = train_feats.reshape(train_feats.shape[0]*train_feats.shape[1], train_feats.shape[2])\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0]*train_labels.shape[1], train_labels.shape[2])\n",
    "    test_feats = test_feats.reshape(test_feats.shape[0]*test_feats.shape[1], test_feats.shape[2])\n",
    "    test_labels = test_labels.reshape(test_labels.shape[0]*test_labels.shape[1], test_labels.shape[2])\n",
    "    val_feats = val_feats.reshape(val_feats.shape[0]*val_feats.shape[1], val_feats.shape[2])\n",
    "    val_labels = val_labels.reshape(val_labels.shape[0]*val_labels.shape[1], val_labels.shape[2])\n",
    "    \n",
    "    if print_out:\n",
    "        print (\"train feats: \", train_feats.shape, \" type: \", type(train_feats))\n",
    "        print (\"train labels: \", train_labels.shape, \" type: \", type(train_labels))\n",
    "\n",
    "        print (\"test feats: \", test_feats.shape, \" type: \", type(test_feats))\n",
    "        print (\"test labels: \", test_labels.shape, \" type: \", type(test_labels))\n",
    "\n",
    "        print (\"val feats: \", val_feats.shape, \" type: \", type(val_feats))\n",
    "        print (\"val labels: \", val_labels.shape, \" type: \", type(val_labels))\n",
    "\n",
    "    # create tensors\n",
    "    train_feats_tensor = torch.tensor(train_feats, requires_grad=True, dtype=torch.float)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.float)\n",
    "\n",
    "    test_feats_tensor = torch.tensor(test_feats, requires_grad=True, dtype=torch.float)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.float)\n",
    "\n",
    "    val_feats_tensor = torch.tensor(val_feats, requires_grad=True, dtype=torch.float)\n",
    "    val_labels_tensor = torch.tensor(val_labels, dtype=torch.float)\n",
    "\n",
    "    if print_out:\n",
    "        print (\"train feats tensor: \", train_feats_tensor.shape, \" type: \", type(train_feats_tensor))\n",
    "        print (\"train labels tensor: \", train_labels_tensor.shape, \" type: \", type(train_labels_tensor))\n",
    "\n",
    "        print (\"test feats tensor: \", test_feats_tensor.shape, \" type: \", type(test_feats_tensor))\n",
    "        print (\"test labels tensor: \", test_labels_tensor.shape, \" type: \", type(test_labels_tensor))\n",
    "\n",
    "        print (\"val feats tensor: \", val_feats_tensor.shape, \" type: \", type(val_feats_tensor))\n",
    "        print (\"val labels tensor: \", val_labels_tensor.shape, \" type: \", type(val_labels_tensor))\n",
    "    \n",
    "    # place tensors on gpu\n",
    "    if place_on_gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            train_feats_tensor = train_feats_tensor.cuda()\n",
    "            train_labels_tensor = train_labels_tensor.cuda()\n",
    "            test_feats_tensor = test_feats_tensor.cuda()\n",
    "            test_labels_tensor = test_labels_tensor.cuda()\n",
    "            val_feats_tensor = val_feats_tensor.cuda()\n",
    "            val_labels_tensor = val_labels_tensor.cuda()\n",
    "            \n",
    "        if print_out:\n",
    "            print (\"train_feats_tensor.device: \", train_feats_tensor.get_device())\n",
    "            print (\"train_labels_tensor.device: \", train_labels_tensor.get_device())\n",
    "            print (\"test_feats_tensor.device: \", test_feats_tensor.get_device())\n",
    "            print (\"test_labels_tensor.device: \", test_labels_tensor.get_device())\n",
    "            print (\"val_feats_tensor.device: \", val_feats_tensor.get_device())\n",
    "            print (\"val_labels_tensor.device: \", val_labels_tensor.get_device())\n",
    "        \n",
    "    return  train_feats_tensor, \\\n",
    "            train_labels_tensor, \\\n",
    "            test_feats_tensor, \\\n",
    "            test_labels_tensor, \\\n",
    "            val_feats_tensor, \\\n",
    "            val_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 set up the data loaders\n",
    "\n",
    "# tensor tuple shape: output of load_datasets()\n",
    "#   [ train_feats_tensor, train_labels_tensor,\n",
    "#     test_feats_tensor, test_labels_tensor,\n",
    "#     val_feats_tensor, val_labels_tensor ]\n",
    "def set_up_dataloaders( batch_size, tensor_tuple):\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(tensor_tuple[0], tensor_tuple[1])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(tensor_tuple[2], tensor_tuple[3])\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle = False)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(tensor_tuple[4], tensor_tuple[5])\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "    \n",
    "    return  train_dataset, train_loader, \\\n",
    "            test_dataset, test_loader, \\\n",
    "            val_dataset, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 bi-LSTM Model Architecture\n",
    "\n",
    "class bi_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers=2, model_type='LSTM'):\n",
    "        super(bi_LSTM, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device=device) # hidden state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(device=device) # cell state\n",
    "        \n",
    "        # print (\"h0 device: \", h0.device)\n",
    "        # print (\"c0 device: \", c0.device)\n",
    "        # print (\"x device: \", x.device)\n",
    "        \n",
    "        x = x[:, None, :]\n",
    "        #print (\"init x shape: \", x.shape)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #print (\"lstm out shape: \", out.shape)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #print (\"linear out shape: \", out.shape)\n",
    "        out = torch.sign(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 helper functions for training\n",
    "\n",
    "def test_network(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # get data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            #print (\"labels: \", labels)\n",
    "            #print (\"pred: \", outputs)\n",
    "            total = labels.shape[0] * labels.shape[1]\n",
    "            correct = 0\n",
    "            for i, frame in enumerate(labels):\n",
    "                #print (i, \" frame: \", frame)\n",
    "                #print (i, \" outputs[i]: \", outputs[i])\n",
    "                for val in torch.eq(frame, outputs[i]):\n",
    "                    if val:\n",
    "                        correct += 1\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "def print_stats(iteration_list, accuracy_list, loss_list):\n",
    "    # final accuracy plot        \n",
    "    plt.plot(iteration_list, accuracy_list)\n",
    "    plt.title(\"accuracy over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # final loss plot        \n",
    "    plt.plot(iteration_list, loss_list)\n",
    "    plt.title(\"loss over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Training\n",
    "\n",
    "#free_gpu_cache()\n",
    "\n",
    "# model parameters:\n",
    "batch_size = 4\n",
    "input_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dim = 9\n",
    "num_layers = 2\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# get dataloaders\n",
    "train_dataset, train_loader, \\\n",
    "test_dataset, test_loader, \\\n",
    "val_dataset, val_loader = set_up_dataloaders(batch_size, load_datasets(print_out=True, place_on_gpu=False))\n",
    "\n",
    "# create model\n",
    "model = bi_LSTM(input_dim, hidden_dim, batch_size, output_dim, num_layers, 'LSTM')\n",
    "model.to(device=device)\n",
    "\n",
    "# print (\"device name: \", torch.cuda.get_device_name(0))\n",
    "# print (\"model.type: \", myModel.model_type)\n",
    "# print (\"model.device: \", next(myModel.parameters()).device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lists for data collection\n",
    "iter = 0\n",
    "delta = 100\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "# perform epochs\n",
    "startTime = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print (\"[ epoch: \", epoch, \"]\")\n",
    "    for batch_index, (feats, labels) in enumerate(train_loader):\n",
    "        # place data on GPU\n",
    "        feats = feats.to(device=device).squeeze(1)\n",
    "        labels = labels.to(device=device)\n",
    "        # print (\"feats shape: \", feats.shape)\n",
    "        # print (\"labels shape: \", labels.shape)\n",
    "        # print (\"labels: \", labels)\n",
    "        \n",
    "        # forward\n",
    "        output = model(feats)\n",
    "        #print (\"output: \", output)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # test accuracy and log stats\n",
    "        if iter % delta == 0:\n",
    "            acc = test_network(model, test_loader)\n",
    "            iteration_list.append(iter)\n",
    "            accuracy_list.append(acc)\n",
    "            loss_list.append(loss)\n",
    "            print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %')\n",
    "    \n",
    "        # increase iteration\n",
    "        iter += 1\n",
    "\n",
    "print (\"time elapsed: \", round((time.time() - startTime), 2), \" sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4872d8d3c2bbc685ce63036ae2af19b2205f6c9572f817ece472ff4ae51ff82f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
