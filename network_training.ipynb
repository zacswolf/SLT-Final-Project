{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 import packages\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from utils.data_loader import data_provider\n",
    "import matplotlib.pyplot as plt\n",
    "from models.bi_lstm import bi_LSTM\n",
    "from models.transformer import Transformer\n",
    "from models.bert_inspired import BertInspired\n",
    "from utils.tools import dotdict\n",
    "from utils.data_loader import DataModule\n",
    "import pytorch_lightning as pl\n",
    "from exp.exp_main import ExpMain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 GPU stuff\n",
    "# device_num = 1\n",
    "# device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"torch device: \", torch.cuda.get_device_name(device))\n",
    "# #device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 helper functions for training\n",
    "\n",
    "# def test_network(model, test_loader):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             # get data\n",
    "#             inputs, labels = data\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(inputs)\n",
    "#             #print (\"labels: \", labels)\n",
    "#             #print (\"pred: \", outputs)\n",
    "#             total = labels.shape[0] * labels.shape[1]\n",
    "#             correct = 0\n",
    "#             for i, frame in enumerate(labels):\n",
    "#                 #print (i, \" frame: \", frame)\n",
    "#                 #print (i, \" outputs[i]: \", outputs[i])\n",
    "#                 for val in torch.eq(frame, outputs[i]):\n",
    "#                     if val:\n",
    "#                         correct += 1\n",
    "            \n",
    "#     return 100 * correct / total\n",
    "\n",
    "# def print_stats(iteration_list, accuracy_list, loss_list):\n",
    "#     # final accuracy plot        \n",
    "#     plt.plot(iteration_list, accuracy_list)\n",
    "#     plt.title(\"accuracy over time\")\n",
    "#     plt.xlabel(\"iterations\")\n",
    "#     plt.ylabel(\"accuracy\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     # final loss plot        \n",
    "#     plt.plot(iteration_list, loss_list)\n",
    "#     plt.title(\"loss over time\")\n",
    "#     plt.xlabel(\"iterations\")\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"bert_inspired\" # \"bert_inspired\" \"transformer\" or \"biLSTM\"\n",
    "\n",
    "model = None\n",
    "if model_type == \"biLSTM\":\n",
    "    # Create model\n",
    "    model_config = dotdict({\n",
    "        \"input_dim\": 128,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"output_dim\": 9,\n",
    "        \"num_layers\": 2,\n",
    "        \"model_type\": model_type\n",
    "    })\n",
    "    # create model\n",
    "    model = bi_LSTM(model_config)\n",
    "    # model.to(device)\n",
    "elif model_type == \"transformer\":\n",
    "    model_config = dotdict({\n",
    "        \"enc_in\": 128,\n",
    "        \"dec_in\": 128,\n",
    "        \"c_out\": 9,\n",
    "        \"d_model\": 128,\n",
    "        \"dropout\": .05,\n",
    "        \"output_attention\": False,\n",
    "        \"n_heads\": 8,\n",
    "        \"d_ff\": None,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"e_layers\": 2,\n",
    "        \"d_layers\": 1,\n",
    "        \"model_type\": model_type\n",
    "    })\n",
    "    model = Transformer(model_config)\n",
    "    # model.to(device)\n",
    "elif model_type == \"bert_inspired\":\n",
    "    model_config = dotdict({\n",
    "        \"enc_in\": (32, 16), # (#windows, # mel filters)\n",
    "        \"c_out\": 9,\n",
    "        \"d_model\": 512,\n",
    "        \"dropout\": .05,\n",
    "        \"output_attention\": False,\n",
    "        \"n_heads\": 8,\n",
    "        \"d_ff\": None,\n",
    "        \"activation\": \"gelu\",\n",
    "        \"e_layers\": 12,\n",
    "        \"model_type\": model_type\n",
    "    })\n",
    "    model = BertInspired(model_config)\n",
    "    # model.to(device)\n",
    "\n",
    "assert model is not None, \"Didn't select a valid model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.0 Training\n",
    "\n",
    "\n",
    "# config = dotdict({\n",
    "#         \"batch_size\": 512,\n",
    "#         \"num_workers\": 0,\n",
    "#         \"seq_len\": 9,\n",
    "#         \"data_id\": \"32x16\"\n",
    "#         \"learning_rate\": 0.0000001,\n",
    "#         \"num_epochs\": 5\n",
    "#     })\n",
    "\n",
    "# train_dataset, train_loader = data_providser(config, flag=\"train\")\n",
    "# val_dataset, val_loader = data_provider(config, flag=\"val\")\n",
    "# test_dataset, test_loader = data_provider(config, flag=\"test\")\n",
    "# # seq_length x num_windows x num_mel_filters\n",
    "\n",
    "# # print (\"device name: \", torch.cuda.get_device_name(0))\n",
    "# # print (\"model.type: \", myModel.model_type)\n",
    "# # print (\"model.device: \", next(myModel.parameters()).device)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# # lists for data collection\n",
    "# iter = 0\n",
    "# delta = 100\n",
    "# iteration_list = []\n",
    "# accuracy_list = []\n",
    "# loss_list = []\n",
    "\n",
    "# # Perform epochs\n",
    "# startTime = time.time()\n",
    "# min_valid_loss = np.inf\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     train_loss = 0.0\n",
    "#     for batch_index, (feats, labels) in enumerate(tqdm(train_loader)):\n",
    "#         feats = feats.float().to(device)\n",
    "#         labels = labels.float().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward\n",
    "#         output = model(feats)\n",
    "#         loss = criterion(output, labels)\n",
    "        \n",
    "#         # Backward\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#     train_loss /= len(train_loader)\n",
    "#     valid_loss = 0.0\n",
    "#     model.eval()     # Optional when not using Model Specific layer\n",
    "#     for batch_index, (feats, labels) in enumerate(tqdm(val_loader)):\n",
    "#         # Transfer Data to GPU if available\n",
    "#         feats = feats.float().to(device)\n",
    "#         labels = labels.float().to(device)\n",
    "#         output = model(feats)\n",
    "#         loss = criterion(output,labels)\n",
    "#         # Calculate Loss\n",
    "#         valid_loss += loss.item()\n",
    "#     valid_loss /= len(val_loader)\n",
    "#     print(f\"Epoch {epoch}\\t\\tTraining Loss: {train_loss}\\t\\tValidation Loss: {valid_loss}\") \n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f\"Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f})\\tSaving The Model\")\n",
    "#         min_valid_loss = valid_loss\n",
    "         \n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "    \n",
    "#     # print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %') \n",
    "#     # print('Test accuracy: %d %%' % (100 * correct / total)) \n",
    "#         # test accuracy and log stats\n",
    "#         # if iter % delta == 0 and iter != 0:\n",
    "#         #     print(\"Testing Network\")\n",
    "#         #     acc = test_network(model, test_loader)\n",
    "#         #     iteration_list.append(iter)\n",
    "#         #     accuracy_list.append(acc)\n",
    "#         #     loss_list.append(loss)\n",
    "#         #     print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %')\n",
    "    \n",
    "#         # # increase iteration\n",
    "#         # iter += 1\n",
    "\n",
    "# print (\"time elapsed: \", round((time.time() - startTime), 2), \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = dotdict({\n",
    "        \"seq_len\": 9,\n",
    "        \"data_id\": \"16x32\",\n",
    "        \"batch_size\": 512,\n",
    "        \"learning_rate\": 0.0000001,\n",
    "        \"max_epochs\": 5\n",
    "    })\n",
    "\n",
    "\n",
    "strategy = \"dp\" # [\"ddp\", \"ddp_spawn\", \"ddp_notebook\", \"ddp_fork\", None]\n",
    "num_workers = os.cpu_count() * (strategy != \"ddp_spawn\")\n",
    "\n",
    "\n",
    "pl.seed_everything(seed=123, workers=True)\n",
    "data_module = DataModule(config, num_workers)\n",
    "\n",
    "\n",
    "# Intantiate Lightning Model\n",
    "exp = ExpMain(model, config)\n",
    "\n",
    "# Create Trainer\n",
    "trainer_params = {\n",
    "    \"max_epochs\":config.max_epochs, \n",
    "    # \"auto_scale_batch_size\": \"power\",\n",
    "    # \"auto_lr_find\": True,\n",
    "    \"logger\": True,\n",
    "    \"accelerator\": \"gpu\", \"devices\": 1, \"auto_select_gpus\": True, \"strategy\": strategy # GPUS\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_params)\n",
    "\n",
    "# Tune model (noop unless auto_scale_batch_size or auto_lr_find)\n",
    "tuner_result = trainer.tune(exp, datamodule=data_module)\n",
    "if \"lr_find\" in tuner_result:\n",
    "    tuner_result[\"lr_find\"].plot(suggest=True)\n",
    "if \"scale_batch_size\" in tuner_result:\n",
    "    print(\"scale_batch_size:\", tuner_result[\"scale_batch_size\"])\n",
    "\n",
    "# Train Model\n",
    "trainer.fit(exp, data_module)\n",
    "\n",
    "# Test Model\n",
    "trainer.test(exp, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "da06df106b6ebc505395780f3c76810614ede012a8d0e2bc265d4156a4243031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
