{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 import packages\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numba import cuda as numba\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from models.bi_lstm import bi_LSTM\n",
    "# from models.transformer import Transformer\n",
    "# from models.bert_inspired import BertInspired\n",
    "from utils.tools import dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 GPU stuff\n",
    "device_num = 1\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"torch device: \", torch.cuda.get_device_name(device))\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# function to clear GPU memory\n",
    "def free_gpu_cache():                        \n",
    "    torch.cuda.empty_cache()\n",
    "    numba.select_device(device_num)\n",
    "    numba.close()\n",
    "    numba.select_device(device_num)\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0 load in datasets\n",
    "\n",
    "def load_datasets(data_id, print_out = False, train_device=\"cpu\", val_device=\"cpu\", test_device=\"cpu\"):\n",
    "    # load files\n",
    "    train_feats = np.load(\"data/dataset\" + data_id + \"/train_feats\" + data_id + \".npy\", allow_pickle=True)\n",
    "    train_labels = np.load(\"data/dataset\" + data_id + \"/train_labels\" + data_id + \".npy\", allow_pickle=True)\n",
    "    test_feats = np.load(\"data/dataset\" + data_id + \"/test_feats\" + data_id + \".npy\", allow_pickle=True)\n",
    "    test_labels = np.load(\"data/dataset\" + data_id + \"/test_labels\" + data_id + \".npy\", allow_pickle=True)\n",
    "    val_feats = np.load(\"data/dataset\" + data_id + \"/val_feats\" + data_id + \".npy\", allow_pickle=True)\n",
    "    val_labels = np.load(\"data/dataset\" + data_id + \"/val_labels\" + data_id + \".npy\", allow_pickle=True)\n",
    "    # reshape numpy arrays\n",
    "    train_feats = train_feats.reshape(train_feats.shape[0]*train_feats.shape[1], train_feats.shape[2])\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0]*train_labels.shape[1], train_labels.shape[2])\n",
    "    test_feats = test_feats.reshape(test_feats.shape[0]*test_feats.shape[1], test_feats.shape[2])\n",
    "    test_labels = test_labels.reshape(test_labels.shape[0]*test_labels.shape[1], test_labels.shape[2])\n",
    "    val_feats = val_feats.reshape(val_feats.shape[0]*val_feats.shape[1], val_feats.shape[2])\n",
    "    val_labels = val_labels.reshape(val_labels.shape[0]*val_labels.shape[1], val_labels.shape[2])\n",
    "    \n",
    "    if print_out:\n",
    "        print(\"train feats: \", train_feats.shape, \" type: \", type(train_feats))\n",
    "        print(\"train labels: \", train_labels.shape, \" type: \", type(train_labels))\n",
    "\n",
    "        print(\"test feats: \", test_feats.shape, \" type: \", type(test_feats))\n",
    "        print(\"test labels: \", test_labels.shape, \" type: \", type(test_labels))\n",
    "\n",
    "        print(\"val feats: \", val_feats.shape, \" type: \", type(val_feats))\n",
    "        print(\"val labels: \", val_labels.shape, \" type: \", type(val_labels))\n",
    "\n",
    "    # create tensors\n",
    "    train_feats_tensor = torch.tensor(train_feats, requires_grad=True, dtype=torch.float).to(train_device)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.float).to(train_device)\n",
    "\n",
    "    test_feats_tensor = torch.tensor(test_feats, requires_grad=True, dtype=torch.float).to(test_device)\n",
    "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.float).to(test_device)\n",
    "\n",
    "    val_feats_tensor = torch.tensor(val_feats, requires_grad=True, dtype=torch.float).to(val_device)\n",
    "    val_labels_tensor = torch.tensor(val_labels, dtype=torch.float).to(val_device)\n",
    "\n",
    "    if print_out:\n",
    "        print (\"train feats tensor: \", train_feats_tensor.shape, \" type: \", type(train_feats_tensor))\n",
    "        print (\"train labels tensor: \", train_labels_tensor.shape, \" type: \", type(train_labels_tensor))\n",
    "\n",
    "        print (\"test feats tensor: \", test_feats_tensor.shape, \" type: \", type(test_feats_tensor))\n",
    "        print (\"test labels tensor: \", test_labels_tensor.shape, \" type: \", type(test_labels_tensor))\n",
    "\n",
    "        print (\"val feats tensor: \", val_feats_tensor.shape, \" type: \", type(val_feats_tensor))\n",
    "        print (\"val labels tensor: \", val_labels_tensor.shape, \" type: \", type(val_labels_tensor))\n",
    "            \n",
    "        if print_out:\n",
    "            print (\"train_feats_tensor.device:\", train_feats_tensor.get_device())\n",
    "            print (\"train_labels_tensor.device:\", train_labels_tensor.get_device())\n",
    "            print (\"test_feats_tensor.device:\", test_feats_tensor.get_device())\n",
    "            print (\"test_labels_tensor.device:\", test_labels_tensor.get_device())\n",
    "            print (\"val_feats_tensor.device:\", val_feats_tensor.get_device())\n",
    "            print (\"val_labels_tensor.device:\", val_labels_tensor.get_device())\n",
    "        \n",
    "    return  (train_feats_tensor,\n",
    "            train_labels_tensor,\n",
    "            test_feats_tensor,\n",
    "            test_labels_tensor,\n",
    "            val_feats_tensor,\n",
    "            val_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 set up the data loaders\n",
    "\n",
    "# tensor tuple shape: output of load_datasets()\n",
    "#   [ train_feats_tensor, train_labels_tensor,\n",
    "#     test_feats_tensor, test_labels_tensor,\n",
    "#     val_feats_tensor, val_labels_tensor ]\n",
    "def set_up_dataloaders(batch_size, tensor_tuple):\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(tensor_tuple[0], tensor_tuple[1])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(tensor_tuple[2], tensor_tuple[3])\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle = False)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(tensor_tuple[4], tensor_tuple[5])\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "    \n",
    "    return  (train_dataset, train_loader,\n",
    "            test_dataset, test_loader,\n",
    "            val_dataset, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 helper functions for training\n",
    "\n",
    "def test_network(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # get data\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            #print (\"labels: \", labels)\n",
    "            #print (\"pred: \", outputs)\n",
    "            total = labels.shape[0] * labels.shape[1]\n",
    "            correct = 0\n",
    "            for i, frame in enumerate(labels):\n",
    "                #print (i, \" frame: \", frame)\n",
    "                #print (i, \" outputs[i]: \", outputs[i])\n",
    "                for val in torch.eq(frame, outputs[i]):\n",
    "                    if val:\n",
    "                        correct += 1\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "def print_stats(iteration_list, accuracy_list, loss_list):\n",
    "    # final accuracy plot        \n",
    "    plt.plot(iteration_list, accuracy_list)\n",
    "    plt.title(\"accuracy over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # final loss plot        \n",
    "    plt.plot(iteration_list, loss_list)\n",
    "    plt.title(\"loss over time\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"biLSTM\" # \"bert_inspired\" \"transformer\" or \"biLSTM\"\n",
    "\n",
    "model = None\n",
    "if model_type == \"biLSTM\":\n",
    "    # Create model\n",
    "    config = dotdict({\n",
    "        \"input_dim\": 128,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"output_dim\": 9,\n",
    "        \"num_layers\": 2,\n",
    "        \"model_type\": model_type\n",
    "    })\n",
    "    # create model\n",
    "    model = bi_LSTM(config)\n",
    "    model.to(device)\n",
    "# elif model_type == \"transformer\":\n",
    "#     # 0 = ????\n",
    "#     config = dotdict({\n",
    "#         \"enc_in\": 128,\n",
    "#         \"dec_in\": 128,\n",
    "#         \"c_out\": 9,\n",
    "#         \"d_model\": 128,\n",
    "#         \"dropout\": .05,\n",
    "#         \"output_attention\": False,\n",
    "#         \"n_heads\": 8,\n",
    "#         \"d_ff\": None,\n",
    "#         \"activation\": \"gelu\",\n",
    "#         \"e_layers\": 2,\n",
    "#         \"d_layers\": 1,\n",
    "#         \"model_type\": model_type\n",
    "#     })\n",
    "#     model = Transformer(config)\n",
    "#     model.to(device)\n",
    "# elif model_type == \"bert_inspired\":\n",
    "#     config = dotdict({\n",
    "#         \"enc_in\": 128,\n",
    "#         \"c_out\": 9,\n",
    "#         \"d_model\": 128,\n",
    "#         \"dropout\": .05,\n",
    "#         \"output_attention\": False,\n",
    "#         \"n_heads\": 8,\n",
    "#         \"d_ff\": None,\n",
    "#         \"activation\": \"gelu\",\n",
    "#         \"e_layers\": 4,\n",
    "#         \"model_type\": model_type\n",
    "#     })\n",
    "#     model = BertInspired(config)\n",
    "#     model.to(device)\n",
    "\n",
    "assert model is not None, \"Didn't select a valid model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Training\n",
    "\n",
    "# free_gpu_cache()\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# get dataloaders\n",
    "train_dataset, train_loader, test_dataset, test_loader, val_dataset, val_loader  = set_up_dataloaders(\n",
    "    batch_size, load_datasets(data_id=\"_5s_50hz\", print_out=True)\n",
    ")\n",
    "\n",
    "# print (\"device name: \", torch.cuda.get_device_name(0))\n",
    "# print (\"model.type: \", myModel.model_type)\n",
    "# print (\"model.device: \", next(myModel.parameters()).device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lists for data collection\n",
    "iter = 0\n",
    "delta = 100\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "# perform epochs\n",
    "startTime = time.time()\n",
    "min_valid_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for batch_index, (feats, labels) in enumerate(tqdm(train_loader)):\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        # place data on GPU\n",
    "        feats = feats.to(device).squeeze(1)\n",
    "        labels = labels.to(device)\n",
    "        print(\"Time consumed in moving feats&labels to gpu: \", time.time() - start)\n",
    "        print (\"feats shape: \", feats.shape)\n",
    "        print (\"labels shape: \", labels.shape)\n",
    "        # print (\"labels: \", labels)\n",
    "        \n",
    "        # forward\n",
    "        start = time.time()\n",
    "        output = model(feats)\n",
    "        loss = criterion(output, labels)\n",
    "        print(\"Time consumed in forward: \", time.time() - start)\n",
    "        #backward\n",
    "        start = time.time()\n",
    "        loss.backward()\n",
    "        print(\"Time consumed in backward: \", time.time() - start)\n",
    "        #gradient descent\n",
    "        start = time.time()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(\"Time consumed in step: \", time.time() - start)\n",
    "    valid_loss = 0.0\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    for batch_index, (feats, labels) in enumerate(tqdm(val_loader)):\n",
    "        # Transfer Data to GPU if available\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(feats)\n",
    "        loss = criterion(output,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "    print(f'Epoch {epoch} \\t\\t Training Loss: {total_loss/ len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}') \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f\\\n",
    "        }--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "    \n",
    "    # print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %') \n",
    "    # print('Test accuracy: %d %%' % (100 * correct / total)) \n",
    "        # test accuracy and log stats\n",
    "        # if iter % delta == 0 and iter != 0:\n",
    "        #     print(\"Testing Network\")\n",
    "        #     acc = test_network(model, test_loader)\n",
    "        #     iteration_list.append(iter)\n",
    "        #     accuracy_list.append(acc)\n",
    "        #     loss_list.append(loss)\n",
    "        #     print(f'\\t iteration: {iter}\\t loss: {loss_list[len(loss_list)-1].item():.3f}\\t accuracy: {accuracy_list[len(accuracy_list)-1]:.3f} %')\n",
    "    \n",
    "        # # increase iteration\n",
    "        # iter += 1\n",
    "\n",
    "print (\"time elapsed: \", round((time.time() - startTime), 2), \" sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2eb717040054f9e1cd6390da57b4c3f6de62338193843f816e8f12a4d5407ba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
